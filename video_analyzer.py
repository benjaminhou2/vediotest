# ä¼˜åŒ–åçš„è§†é¢‘åˆ†æå¼•æ“
import cv2
import numpy as np
import os
import sys
import time
import traceback
import imagehash
from PIL import Image
from moviepy.editor import VideoFileClip
from pydub import AudioSegment
from scipy.spatial.distance import cosine
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import json
from datetime import datetime

from config import config
from utils import (
    validate_video_file, get_file_info, calculate_file_hash,
    clean_temp_files, format_duration, ProgressTracker,
    similarity_to_percentage, calculate_overall_similarity,
    is_similar, safe_remove_file, create_memory_monitor
)

class VideoAnalyzer:
    """ä¼˜åŒ–åçš„è§†é¢‘åˆ†æå™¨"""
    
    def __init__(self):
        self.config = config
        self.memory_monitor = create_memory_monitor()
        self.progress_lock = Lock()
        
        # åŠ è½½CNNæ¨¡å‹
        print("ğŸ”„ åŠ è½½ResNet50æ¨¡å‹...")
        self.resnet_model = ResNet50(weights="imagenet", include_top=False, pooling="avg")
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶
        clean_temp_files(self.config.files.temp_dir)
    
    def extract_frames_parallel(self, video_path: str, interval: int = 1, max_frames: int = None) -> list:
        """
        å¹¶è¡Œæå–è§†é¢‘å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            interval: å¸§é—´éš”(ç§’)
            max_frames: æœ€å¤§å¸§æ•°é™åˆ¶
        
        Returns:
            æå–çš„å¸§åˆ—è¡¨
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        # è®¡ç®—è¦æå–çš„å¸§ç´¢å¼•
        frame_indices = []
        frame_step = int(fps * interval)
        for i in range(0, total_frames, frame_step):
            frame_indices.append(i)
            if max_frames and len(frame_indices) >= max_frames:
                break
        
        print(f"ğŸ“¹ è§†é¢‘æ—¶é•¿: {format_duration(duration)}, å°†æå– {len(frame_indices)} å¸§")
        
        frames = []
        progress = ProgressTracker(len(frame_indices), f"æå–å¸§ ({os.path.basename(video_path)})")
        
        for idx, frame_idx in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame_rgb))
            
            progress.update(1, f"å¸§ {idx+1}")
            
            # å†…å­˜ç›‘æ§
            if idx % 10 == 0:
                memory_mb = self.memory_monitor()
                if memory_mb > 1000:  # è¶…è¿‡1GBè­¦å‘Š
                    print(f"\nâš ï¸  å†…å­˜ä½¿ç”¨: {memory_mb:.1f}MB")
        
        progress.finish()
        cap.release()
        return frames
    
    def phash_image(self, img: Image.Image) -> str:
        """è®¡ç®—å›¾åƒæ„ŸçŸ¥å“ˆå¸Œ"""
        try:
            return str(imagehash.phash(img, hash_size=self.config.processing.hash_size))
        except Exception as e:
            print(f"âš ï¸  pHashè®¡ç®—å¤±è´¥: {e}")
            return "0" * (self.config.processing.hash_size ** 2)
    
    def compare_hashes_parallel(self, hashes1: list, hashes2: list) -> float:
        """å¹¶è¡Œæ¯”è¾ƒå“ˆå¸Œå€¼"""
        def calculate_distance(h1, h2):
            try:
                # å°†å­—ç¬¦ä¸²å“ˆå¸Œè½¬æ¢ä¸ºimagehashå¯¹è±¡è¿›è¡Œæ¯”è¾ƒ
                hash1 = imagehash.hex_to_hash(h1)
                hash2 = imagehash.hex_to_hash(h2)
                return hash1 - hash2
            except:
                return 64  # æœ€å¤§è·ç¦»
        
        with ThreadPoolExecutor(max_workers=self.config.processing.max_workers) as executor:
            futures = [executor.submit(calculate_distance, h1, h2) 
                      for h1, h2 in zip(hashes1, hashes2)]
            
            distances = []
            for future in as_completed(futures):
                try:
                    distances.append(future.result())
                except Exception as e:
                    print(f"âš ï¸  å“ˆå¸Œæ¯”è¾ƒå¤±è´¥: {e}")
                    distances.append(64)
        
        return np.mean(distances) if distances else 64
    
    def extract_audio_hash_safe(self, video_path: str) -> np.ndarray:
        """å®‰å…¨æå–éŸ³é¢‘å“ˆå¸Œ"""
        temp_audio_path = os.path.join(self.config.files.temp_dir, 
                                     f"temp_audio_{int(time.time())}.wav")
        
        try:
            # æå–éŸ³é¢‘
            clip = VideoFileClip(video_path)
            if clip.audio is None:
                print("âš ï¸  è§†é¢‘æ— éŸ³é¢‘è½¨é“")
                return np.zeros(512)
            
            clip.audio.write_audiofile(temp_audio_path, logger=None, verbose=False)
            clip.close()
            
            # å¤„ç†éŸ³é¢‘
            audio = AudioSegment.from_wav(temp_audio_path)
            samples = np.array(audio.get_array_of_samples())
            
            # é™åˆ¶é‡‡æ ·æ•°é¿å…å†…å­˜é—®é¢˜
            if len(samples) > 1000000:  # 1Mé‡‡æ ·ç‚¹
                samples = samples[:1000000]
            
            freq = np.fft.fft(samples)
            spectrum_hash = np.sign(np.real(freq[:512]))
            
            return spectrum_hash
            
        except Exception as e:
            print(f"âš ï¸  éŸ³é¢‘æå–å¤±è´¥: {e}")
            return np.zeros(512)
        finally:
            safe_remove_file(temp_audio_path)
    
    def compare_audio_hash(self, a1: np.ndarray, a2: np.ndarray) -> float:
        """æ¯”è¾ƒéŸ³é¢‘å“ˆå¸Œ"""
        try:
            if len(a1) == 0 or len(a2) == 0:
                return 1.0
            return np.sum(a1 != a2) / len(a1)
        except:
            return 1.0
    
    def extract_cnn_features_batch(self, images: list) -> list:
        """æ‰¹é‡æå–CNNç‰¹å¾"""
        features = []
        batch_size = 8  # æ§åˆ¶æ‰¹å¤„ç†å¤§å°é¿å…å†…å­˜é—®é¢˜
        
        progress = ProgressTracker(len(images), "æå–CNNç‰¹å¾")
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size]
            batch_features = []
            
            try:
                # é¢„å¤„ç†æ‰¹é‡å›¾åƒ
                batch_arrays = []
                for img in batch:
                    img_resized = img.resize(self.config.processing.cnn_image_size)
                    x = image.img_to_array(img_resized)
                    x = preprocess_input(x)
                    batch_arrays.append(x)
                
                if batch_arrays:
                    batch_input = np.stack(batch_arrays)
                    batch_pred = self.resnet_model.predict(batch_input, verbose=0)
                    
                    for pred in batch_pred:
                        batch_features.append(pred.flatten())
                
                features.extend(batch_features)
                
            except Exception as e:
                print(f"âš ï¸  CNNç‰¹å¾æå–å¤±è´¥: {e}")
                # æ·»åŠ é›¶ç‰¹å¾å‘é‡ä½œä¸ºå¤‡ç”¨
                for _ in batch:
                    features.append(np.zeros(2048))  # ResNet50è¾“å‡ºç‰¹å¾ç»´åº¦
            
            progress.update(len(batch))
        
        progress.finish()
        return features
    
    def compare_cnn_features_parallel(self, f1_list: list, f2_list: list) -> float:
        """å¹¶è¡Œæ¯”è¾ƒCNNç‰¹å¾"""
        def calculate_cosine(f1, f2):
            try:
                if len(f1) == 0 or len(f2) == 0:
                    return 1.0
                return cosine(f1, f2)
            except:
                return 1.0
        
        with ThreadPoolExecutor(max_workers=self.config.processing.max_workers) as executor:
            futures = [executor.submit(calculate_cosine, f1, f2) 
                      for f1, f2 in zip(f1_list, f2_list)]
            
            distances = []
            for future in as_completed(futures):
                try:
                    distances.append(future.result())
                except Exception as e:
                    print(f"âš ï¸  ç‰¹å¾æ¯”è¾ƒå¤±è´¥: {e}")
                    distances.append(1.0)
        
        return np.mean(distances) if distances else 1.0
    
    def analyze_video_similarity(self, video_path_1: str, video_path_2: str, 
                               progress_callback=None) -> dict:
        """
        åˆ†æä¸¤ä¸ªè§†é¢‘çš„ç›¸ä¼¼åº¦
        
        Args:
            video_path_1: ç¬¬ä¸€ä¸ªè§†é¢‘è·¯å¾„
            video_path_2: ç¬¬äºŒä¸ªè§†é¢‘è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # éªŒè¯æ–‡ä»¶
        for i, video_path in enumerate([video_path_1, video_path_2], 1):
            valid, error = validate_video_file(video_path, self.config.files.max_file_size_mb)
            if not valid:
                raise ValueError(f"è§†é¢‘{i}éªŒè¯å¤±è´¥: {error}")
        
        print(f"\nğŸ¬ å¼€å§‹åˆ†æè§†é¢‘ç›¸ä¼¼åº¦")
        print(f"ğŸ“ è§†é¢‘1: {os.path.basename(video_path_1)}")
        print(f"ğŸ“ è§†é¢‘2: {os.path.basename(video_path_2)}")
        
        result = {
            'video1_info': get_file_info(video_path_1),
            'video2_info': get_file_info(video_path_2),
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'config_used': {
                'frame_interval': self.config.processing.frame_interval,
                'hash_size': self.config.processing.hash_size,
                'cnn_image_size': self.config.processing.cnn_image_size
            }
        }
        
        try:
            # 1. æå–è§†é¢‘å¸§
            if progress_callback:
                progress_callback("æå–è§†é¢‘å¸§", 10)
            
            frames_1 = self.extract_frames_parallel(video_path_1, self.config.processing.frame_interval)
            frames_2 = self.extract_frames_parallel(video_path_2, self.config.processing.frame_interval)
            
            # ç»Ÿä¸€å¸§æ•°
            min_len = min(len(frames_1), len(frames_2))
            if min_len == 0:
                raise ValueError("æ— æ³•æå–è§†é¢‘å¸§")
            
            frames_1 = frames_1[:min_len]
            frames_2 = frames_2[:min_len]
            
            print(f"ğŸ“Š ä½¿ç”¨ {min_len} å¸§è¿›è¡Œåˆ†æ")
            
            # 2. pHashåˆ†æ
            if progress_callback:
                progress_callback("è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ", 30)
            
            print("ğŸ” è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ...")
            hashes_1 = [self.phash_image(f) for f in frames_1]
            hashes_2 = [self.phash_image(f) for f in frames_2]
            hash_distance = self.compare_hashes_parallel(hashes_1, hashes_2)
            
            # 3. CNNç‰¹å¾åˆ†æ
            if progress_callback:
                progress_callback("æå–CNNç‰¹å¾", 60)
            
            print("ğŸ§  æå–CNNç‰¹å¾...")
            features_1 = self.extract_cnn_features_batch(frames_1)
            features_2 = self.extract_cnn_features_batch(frames_2)
            cnn_distance = self.compare_cnn_features_parallel(features_1, features_2)
            
            # 4. éŸ³é¢‘åˆ†æ
            if progress_callback:
                progress_callback("åˆ†æéŸ³é¢‘", 80)
            
            print("ğŸµ åˆ†æéŸ³é¢‘...")
            audio_hash_1 = self.extract_audio_hash_safe(video_path_1)
            audio_hash_2 = self.extract_audio_hash_safe(video_path_2)
            audio_diff = self.compare_audio_hash(audio_hash_1, audio_hash_2)
            
            # 5. è®¡ç®—ç›¸ä¼¼åº¦ç»“æœ
            if progress_callback:
                progress_callback("ç”ŸæˆæŠ¥å‘Š", 95)
            
            # åŸºç¡€è·ç¦»æŒ‡æ ‡
            result.update({
                'pHash_distance': float(hash_distance),
                'cnn_cosine_distance': float(cnn_distance),
                'audio_difference_ratio': float(audio_diff)
            })
            
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
            result.update({
                'phash_similarity_percent': similarity_to_percentage(hash_distance, 'phash'),
                'cnn_similarity_percent': similarity_to_percentage(cnn_distance, 'cnn'),
                'audio_similarity_percent': similarity_to_percentage(audio_diff, 'audio')
            })
            
            # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
            overall_similarity = calculate_overall_similarity(hash_distance, cnn_distance, audio_diff)
            result['overall_similarity_percent'] = overall_similarity
            
            # ç›¸ä¼¼æ€§åˆ¤æ–­
            is_similar_result = is_similar(result, {
                'phash_threshold': self.config.thresholds.phash_threshold,
                'cnn_threshold': self.config.thresholds.cnn_threshold,
                'audio_threshold': self.config.thresholds.audio_threshold,
                'overall_threshold': self.config.thresholds.overall_threshold
            })
            result['is_similar'] = is_similar_result
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            result['processing_time_seconds'] = processing_time
            
            if progress_callback:
                progress_callback("å®Œæˆ", 100)
            
            print(f"\nâœ… åˆ†æå®Œæˆ - ç”¨æ—¶: {format_duration(processing_time)}")
            print(f"ğŸ“Š ç»¼åˆç›¸ä¼¼åº¦: {overall_similarity:.1f}%")
            print(f"ğŸ¯ åˆ¤æ–­ç»“æœ: {'ç›¸ä¼¼' if is_similar_result else 'ä¸ç›¸ä¼¼'}")
            
            return result
            
        except Exception as e:
            error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
            
            result.update({
                'error': error_msg,
                'traceback': traceback.format_exc(),
                'processing_time_seconds': time.time() - start_time
            })
            return result
    
    def generate_detailed_report(self, result: dict, output_path: str = None) -> str:
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = os.path.join(self.config.files.output_dir, 
                                     f"similarity_report_{timestamp}.json")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
            return ""

def main():
    """å‘½ä»¤è¡Œä¸»ç¨‹åº"""
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python video_analyzer.py <è§†é¢‘1> <è§†é¢‘2>")
        sys.exit(1)
    
    video1_path = sys.argv[1]
    video2_path = sys.argv[2]
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = VideoAnalyzer()
    
    try:
        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze_video_similarity(video1_path, video2_path)
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š è§†é¢‘ç›¸ä¼¼åº¦åˆ†æç»“æœ")
        print("="*60)
        
        if 'error' in result:
            print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
            return
        
        print(f"ğŸ¬ è§†é¢‘1: {result['video1_info']['name']}")
        print(f"ğŸ¬ è§†é¢‘2: {result['video2_info']['name']}")
        print()
        
        print("ğŸ“ˆ ç›¸ä¼¼åº¦æŒ‡æ ‡:")
        print(f"  â€¢ è§†è§‰ç›¸ä¼¼åº¦ (pHash): {result['phash_similarity_percent']:.1f}%")
        print(f"  â€¢ è¯­ä¹‰ç›¸ä¼¼åº¦ (CNN):   {result['cnn_similarity_percent']:.1f}%")
        print(f"  â€¢ éŸ³é¢‘ç›¸ä¼¼åº¦:         {result['audio_similarity_percent']:.1f}%")
        print(f"  â€¢ ç»¼åˆç›¸ä¼¼åº¦:         {result['overall_similarity_percent']:.1f}%")
        print()
        
        print(f"ğŸ¯ åˆ¤æ–­ç»“æœ: {'âœ… ç›¸ä¼¼' if result['is_similar'] else 'âŒ ä¸ç›¸ä¼¼'}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {format_duration(result['processing_time_seconds'])}")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        if config.reports.generate_detailed:
            report_path = analyzer.generate_detailed_report(result)
            if report_path:
                print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    main()